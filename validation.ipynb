{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validate checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "command = [\n",
    "    \"python3\",\n",
    "     \"tests/validate_dataset_checkpoint.py\"\n",
    "]\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test run on updated config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(command, stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Print output in real-time\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m process\u001b[38;5;241m.\u001b[39mstderr:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Wait for the process to finish\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "config_file = \"audioldm_train/config/audioldm_original_with_imagebind/audioldm_with_imagebind.yaml\"\n",
    "command = [\n",
    "    \"python3\", \n",
    "    \"audioldm_train/train/latent_diffusion.py\", \n",
    "    \"-c\", config_file,\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Print output in real-time\n",
    "for line in process.stdout:\n",
    "    print(line, end=\"\")\n",
    "\n",
    "# Wait for the process to finish\n",
    "process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing the imagebind model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagebind.imagebind import data\n",
    "import torch\n",
    "from imagebind.imagebind.models import imagebind_model\n",
    "from imagebind.imagebind.models.imagebind_model import ModalityType\n",
    "\n",
    "# text_list=[\"A dog.\", \"A car\", \"A bird\"]\n",
    "video_paths=[\"data/dataset/videoset/scratch/shared/beegfs/hchen/train_data/VGGSound_final/video/-__L3T1Yv_4_000000.mp4\",\n",
    "             \"data/dataset/videoset/scratch/shared/beegfs/hchen/train_data/VGGSound_final/video/-_-SejUs_VI_000063.mp4\"]\n",
    "# audio_paths=[\".assets/dog_audio.wav\", \".assets/car_audio.wav\", \".assets/bird_audio.wav\"]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Load data\n",
    "inputs = {\n",
    "    ModalityType.VISION: data.load_and_transform_video_data(video_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model(inputs)\n",
    "\n",
    "print(embeddings['vision'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing the clap classifier free coditional class to check it's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audioldm_train.conditional_models  import CLAPAudioEmbeddingClassifierFreev2\n",
    "\n",
    "model = CLAPAudioEmbeddingClassifierFreev2(\n",
    "    pretrained_path=\"data/checkpoints/clap_htsat_tiny.pt\",\n",
    "    embed_mode=\"test\",\n",
    "    amodel=\"HTSAT-tiny\",\n",
    "    unconditional_prob=1\n",
    "    )\n",
    "# rand_px = torch.rand(1)\n",
    "uncond_emb = model.get_unconditional_condition(batchsize=2)\n",
    "build_emb = model.build_unconditional_emb()\n",
    "uncond_emb.squeeze()\n",
    "print(\"The unconditional shape is \" , uncond_emb.shape)\n",
    "print(\"the shape of the clap build emb is \" , build_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video to audio inferance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 audioldm_train/infer.py --config_yaml audioldm_train/config/audioldm_original_with_imagebind/audioldm_with_imagebind.yaml --list_inference tests/captionlist/inference_test.lst --reload_from_ckpt log/latent_diffusion/audioldm_original_with_imagebind/audioldm_with_imagebind/checkpoints/checkpoint-fad-133.00-global_step=244999.ckpt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioldm_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
